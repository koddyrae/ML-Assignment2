# -*- coding: utf-8 -*-
"""Copy of Assignment2 Description.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BEkFWFCbgXUwkySPgcmy9AQqn2RZ76dK

**Model Training** - The purpose of this assignment is to apply your knowledge of machine learning to **two** different datasets. For each dataset, you will train a model to achieve a reasonable performance. You may use trial and error process or search strategies (e.g., Grid search) to fine-tune your model. If you are making any choice or trade-off, document it in your notebook. Documentations should be added as a text cell to your notebook.

**Presentation** - Create a summary of your findings and present it in a format of your choice (short video, short report, a few presentation slides, etc.)

**Reflection** - Finally, add your contribution statement and self assessment to your notebook.

# Your Tasks

## Mental Disorder Dataset

Mental Health is an important issue. It must be taken seriously and treated appropriately. Using historical data collected from people diagnosed with Mania Bipolar Disorder (Bipolar Type-1), Depressive Bipolar Disorder (Bipolar Type-2), Major Depressive Disorder, and Normal Individuals, your task is to build a model that can detect the type of mental disorder. The Normal category refers to the individuals who may have minor mental problems, but they differ from those suffering from Major Depressive Disorder and Bipolar Disorder.

Your model will be used in as a tool that will help psychiatrists in their decision making process.

A collection of 120 Psychology Patients with 17 Essential Symptoms is prepared.
The dataset contains the 17 essential symptoms psychiatrists use to diagnose the described disorders. The behavioral symptoms are considered the levels of patients Sadness, Exhaustness, Euphoric, Sleep disorder, Mood swings, Suicidal thoughts, Anorexia, Anxiety, Try-explaining, Nervous breakdown, Ignore & Move-on, Admitting mistakes, Overthinking, Aggressive response, Optimism, Sexual activity, and Concentration.

### Install and import dependencies
"""

import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""### Download dataset"""

dataframe = pd.read_csv("/Mental-Health.csv")

dataframe.head()

plt.hist(dataframe["Expert Diagnose"])
plt.xlabel("Disorder")
plt.ylabel("Frequency")

plt.show()

#### Data Preprocessing: Replacing Categorical Values with Numerical Values
dataframe['Sexual Activity'].replace(['1 From 10', '2 From 10', '3 From 10', '4 From 10', '5 From 10', '6 From 10', '7 From 10', '8 From 10', '9 From 10'],[1, 2, 3, 4, 5, 6, 7, 8, 9], inplace=True)
dataframe['Optimisim'].replace(['1 From 10', '2 From 10', '3 From 10', '4 From 10', '5 From 10', '6 From 10', '7 From 10', '8 From 10', '9 From 10'],[1, 2, 3, 4, 5, 6, 7, 8, 9], inplace=True)
dataframe['Concentration'].replace(['1 From 10', '2 From 10', '3 From 10', '4 From 10', '5 From 10', '6 From 10', '7 From 10', '8 From 10', '9 From 10'],[1, 2, 3, 4, 5, 6, 7, 8, 9], inplace=True)

### Data Preprocessing: Separating Target Variable and Features. This is the variable we aim to predict.

target = dataframe["Expert Diagnose"]
data = dataframe.drop("Expert Diagnose", axis = 1)

from imblearn.under_sampling import RandomUnderSampler
sampler = RandomUnderSampler(random_state=0)

# Perform undersampling to address the unbalanced dataset
X_under, y_under = sampler.fit_resample(data, target)

# Update data and target with the undersampled data
data = X_under
target = y_under

target.value_counts()

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)

y_train.value_counts()

y_test.value_counts()

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer

num_attribs = ["Optimisim", "Concentration", "Sexual Activity"]
cat_attribs = ["Sadness", "Euphoric", "Exhausted", "Sleep dissorder", "Mood Swing", "Suicidal thoughts", "Anorxia", "Authority Respect", "Try-Explanation", "Aggressive Response", "Ignore & Move-On", "Nervous Break-down", "Admit Mistakes", "Overthinking"]

# Numerical pipeline
num_pipeline = make_pipeline(
    SimpleImputer(strategy="median"),
    StandardScaler())

# Categorical pipelineg
cat_pipeline = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

# Full pipeline combining numerical and categorical pipelines
full_pipeline = ColumnTransformer([
    ("num", num_pipeline, num_attribs),
    ("cat", cat_pipeline, cat_attribs),
])

X_train_prepared = full_pipeline.fit_transform(X_train)

from sklearn.neighbors import KNeighborsClassifier

# Initialize KNN classifier with n_neighbors=4
# Setting it to 4 means that the model will consider the class labels of the four nearest neighbors
clf_1 = KNeighborsClassifier(n_neighbors=4)

# Train the classifier
clf_1.fit(X_train_prepared, y_train)

clf_1.classes_

from sklearn.model_selection import cross_val_score

# Perform cross-validation on the KNN classifier
clf_1_scores = cross_val_score(clf_1, X_train_prepared, y_train, cv=5)
pd.Series(clf_1_scores).describe()

from sklearn.tree import DecisionTreeClassifier

# Initialize Decision Tree classifier
clf_2 = DecisionTreeClassifier(random_state=42)

# Train the classifier
clf_2.fit(X_train_prepared, y_train)

clf_2.classes_

print(clf_2.feature_importances_)

# Perform cross-validation on the Decision Tree classifier
clf_2_scores = cross_val_score(clf_2, X_train_prepared, y_train, cv=5)
pd.Series(clf_2_scores).describe()

from sklearn.ensemble import RandomForestClassifier

# Initialize Random Forest classifier with random_state=42

#forest_reg = make_pipeline(preprocessing, RandomForestRegressor(random_state=42))
clf_3 = RandomForestClassifier(random_state=42)

# Perform cross-validation on the Random Forest classifier
forest_rmses = cross_val_score(clf_3, X_train_prepared, y_train, cv=5)
pd.Series(forest_rmses).describe()

from sklearn.model_selection import GridSearchCV

# Define the parameter grid for hyperparameter tuning

param_grid = [
    {
        'n_estimators': [3, 10, 30],
        'max_features': [4, 6, 8]
     },

    {
        'bootstrap': [False],
        'n_estimators': [3, 10],
        'max_features': [2,3,4]
     },
]

# Initialize GridSearchCV with the Random Forest classifier (clf_3) to find the best combination of hyperparameters that maximizes the model's performance.
grid_search = GridSearchCV(clf_3, param_grid, cv=3, return_train_score=True)

# Perform grid search on the training data
grid_search.fit(X_train_prepared, y_train)

grid_search.best_estimator_

# Assign the best estimator from grid search to final_model
final_model = grid_search.best_estimator_

# Extract feature importances from the final model
feature_importances = final_model.feature_importances_
feature_importances.round(2)

# Transform the test data using the full pipeline
X_test_prepared = full_pipeline.transform(X_test)

# Make predictions using the final model
y_predictions = final_model.predict(X_test_prepared)

# Display the predictions
y_predictions

# Predict class probabilities for the test data
y_predictions_prob = final_model.predict_proba(X_test_prepared)
y_predictions_prob

# Count the number of instances for each class in the test set

print("Bipolar Type-2", str(len(y_test[y_test=="Bipolar Type-2"])))
print("Bipolar Type-1", str(len(y_test[y_test=="Bipolar Type-1"])))
print("Depression", str(len(y_test[y_test=="Depression"])))
print("Normal", str(len(y_test[y_test=="Normal"])))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Compute the confusion matrix and plots it
cm = confusion_matrix(y_test, y_predictions, labels=["Bipolar Type-2", "Bipolar Type-1", "Depression", "Normal"])
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=["Bipolar Type-2", "Bipolar Type-1", "Depression", "Normal"])
disp.plot()

from sklearn.metrics import accuracy_score

# Calculate the accuracy score
accuracy_score(y_test, y_predictions)

from sklearn.metrics import precision_score

# Calculate the precission score
precision_score(y_test, y_predictions, average='weighted')

from sklearn.metrics import recall_score

# Calculate the recall score
recall_score(y_test, y_predictions, average='weighted')

from sklearn.metrics import classification_report
target_names = ["Bipolar Type-2", "Bipolar Type-1", "Depression", "Normal"]
print(classification_report(y_test, y_predictions, target_names=target_names))

import joblib

joblib.dump(final_model, "Mental_Disorder_CLF_model.pkl")

"""## Wine Quality dataset

The [Wine Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality) dataset consists of a set of 11 input parameters (acidity, sugar content, alcohol level, etc) and a single output ("quality" as assessed by expert tasters). The white wine dataset, with a total of 4898 samples, is selected for this problem.

The wine quality dataset is larger than the mental health disrder datasets, there are a number of challenges to overcome:
- Not all inputs are necessarily relevant, and some may be correlated. Part of your experimentation should be determining which inputs to use.
- The dataset is not pre-split into test/train/validation sets.
- The input parameters are not on the same scale, so they require preprocessing.
- The output can be treated as either a regression problem (predict a continuous value from 0 to 9) or as a classification problem.
- The output classes are unbalanced. You will have to address this through appropriate choice of loss metric or resampling.

Train a regression model OR a 3-class classifier (low, medium, high quality). To train a 3-class classifier, you must re-label the data. You may choose the cut-off points for each class.
"""

white_wine_csv = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'
wine_data = pd.read_csv(white_wine_csv, sep=';')

# Inspect the dataset
wine_data.info()
wine_data.head()

"""Note that the wine quality dataset is **unbalanced**, with many more samples in the "medium" quality categories than at the extremes."""

plt.hist(wine_data['quality'], bins=10)
plt.show()

#Preprocessing the data

wine_data['quality'].value_counts()

wine_data['quality'].unique()

# Data Preprocessing

# Creates a feature called "alcohol_cat" based on the "alcohol" column in wine_data. It bins the alcohol levels into five categories using the pd.cut function, with predefined bins and corresponding labels.

wine_data["alcohol_cat"] = pd.cut(wine_data["alcohol"],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])

corr_matrix = wine_data.corr()
corr_matrix

# features that are most strongly correlated (positively or negatively) with wine quality.
corr_matrix["quality"].sort_values(ascending=False)

# Removing Irrelevant features due to correlation.
wine_data1 = wine_data.drop(['sulphates', 'free sulfur dioxide', 'citric acid'], axis=1)
wine_data1.head()

# The dataset is then split into training and testing sets using stratified sampling based on the "alcohol_cat" column. The test set makes up 20% of the total dataset.
from sklearn.model_selection import train_test_split

target = wine_data1['quality']
data = wine_data1.drop('quality', axis=1)

strat_train_set, strat_test_set = train_test_split(wine_data1, test_size=0.2, stratify=wine_data1["quality"], random_state=42)

# Extract features and target variables from the split sets
X_train = strat_train_set.drop("quality", axis=1)
y_train = strat_train_set["quality"]

X_test = strat_test_set.drop("quality", axis=1)
y_test = strat_test_set["quality"]

# Apply oversampling to the training set
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=42)
X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)

#Splitting the X_train resampled and the y_train_resampled so we can create a validation split to test the different models and choose the best one
X_train_resampled, X_val_resampled, y_train_resampled, y_val_resampled = train_test_split(
    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42
)

X_train_resampled.value_counts()

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer

# Data pipeline to correct all the data to numerical
num_attribs1 = ["fixed acidity", "volatile acidity", "residual sugar", "chlorides",
               "total sulfur dioxide", "density", "pH", "alcohol"]

num_pipeline1 = make_pipeline(
    SimpleImputer(strategy="median"),
    StandardScaler())

full_pipeline1 = ColumnTransformer([
    ("num", num_pipeline1, num_attribs1),
])

X_prepared = full_pipeline1.fit_transform(X_train_resampled)

# Making sure that the X_val_resampled is the same shape as X_prepared so it works with the model
X_val_prepared = full_pipeline1.transform(X_val_resampled)

"""This step runs the validation data through a series of models to test, which would be most suitable for our dataset. There are a total of 16 models that we are testing through, so it may take some time for this to complete

***WARNING***<BR>
This step will take a couple minutes to complete all of the calculations.
"""

from sklearn.linear_model import LinearRegression, BayesianRidge, Ridge, Lasso, ElasticNet, HuberRegressor, PassiveAggressiveRegressor, RANSACRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

results = {}

# List of models
models = [
    LinearRegression(),
    DecisionTreeRegressor(random_state=42),
    RandomForestRegressor(random_state=42),
    BayesianRidge(),
    GradientBoostingRegressor(random_state=42),
    KNeighborsRegressor(),
    Ridge(),
    Lasso(),
    ElasticNet(),
    HuberRegressor(),
    PassiveAggressiveRegressor(),
    RANSACRegressor(),
    SVR()
]

# Evaluate all models using a loop
for model in models:
    print(f"\nEvaluating {model.__class__.__name__}:\n")

    # Fit the model
    model.fit(X_prepared, y_train_resampled)

    # Make predictions
    y_pred = model.predict(X_val_prepared)

    # Calculate metrics
    mse = mean_squared_error(y_val_resampled, y_pred)
    mae = mean_absolute_error(y_val_resampled, y_pred)
    r2 = r2_score(y_val_resampled, y_pred)

    # Perform cross-validation
    cv_scores = -cross_val_score(model, X_val_prepared, y_val_resampled, scoring="neg_root_mean_squared_error", cv=10)
    mean_cv_score = np.mean(cv_scores)

    # Store results in the dictionary
    results[model.__class__.__name__] = {'MSE': mse, 'MAE': mae, 'R2': r2, 'CV_RMSE': mean_cv_score}

    # Print results for the current model
    for metric, value in results[model.__class__.__name__].items():
        print(f"{metric}: {value}")

#From looking at the model choosing process, it looks like RandomForestRegressor had the best performance on the data, so we will be moving forward with it
ran_for = RandomForestRegressor(random_state=42)
ran_for.fit(X_prepared, y_train_resampled)

#Fine Tuning the model with grid search
from sklearn.model_selection import GridSearchCV

param_grid = [
    {
        'n_estimators': [3, 10, 30],
        'max_features': [4, 6, 8]
     },

    {
        'bootstrap': [False],
        'n_estimators': [3, 10],
        'max_features': [2,3,4]
     },
]

grid_search = GridSearchCV(ran_for, param_grid, cv=3, return_train_score=True)
grid_search.fit(X_prepared, y_train_resampled)

grid_search.best_params_

#Creating the model with the best parameters
final_model = grid_search.best_estimator_

feature_importances = final_model.feature_importances_
feature_importances.round(2)

#Final testing and rmse calculations
X_test_prepared = full_pipeline1.transform(X_test)

final_predictions = final_model.predict(X_test_prepared)

final_rmse = mean_squared_error(y_test, final_predictions, squared=False)
print(final_rmse)

final_predictions

import joblib

joblib.dump(final_model, "my_wine_quality_model.pkl")

"""## Reflection

**Contribution statement**: Briefly explain the contribution of each member to the assignment.

**Koddy**: I worked mainly on dataset 2 (Wine Quality), specifically oversampling the data and splitting it properly into test/train/validation sets, and testing and evaluating the different models.
<br>
**Alishan**: I worked on dataset 1 (Mental Health), which included preprocessing the data, evaluating different models, and final findings within the dataset.
<br>
**Aldo**: I completed the in text documentation of Task One and Two. Also helped split data for Task 2 completed the report for Task 2.
<br>
**Paraspreet**: I worked on the report for Task 1 and Task 2, while checking over the evaluation and choice of models for both tasks
<br>

**Self assessment**: In your opinion, what are the deficiencies in your work that need to be improved?

- In my opinion, the decision tree model should also be looked at to see if it had better performance on the data when it was actually trained in comparison to the RandomForestRegressor model that we ultimately went with, since the model also had similar scores in the evaluation testing.
- Look into different ways to resample the dataset when it is imbalanced to try to cause less issues like overfitting.

# Marking Rubric

The following table provides a summary of how your assignment will be marked. Your results will not be ranked against each other; what counts is reasonable choices and description of your thought processes.

<table>
<tr>
<th>Item</th>
<th>Points</th>
</tr>

<tr>
<td>The training process for dataset 1</td>
<td>20</td>
</tr>

<tr>
<td>The training process for dataset 2</td>
<td>24</td>
</tr>

<tr>
<td>Documentation of the process and choices made </td>
<td>20</td>
</tr>

<tr>
<td>Overall code quality</td>
<td>10</td>
</tr>

<tr>
<td>Presentation</td>
<td>16</td>
</tr>

<tr>
<td>Reflection</td>
<td>10</td>
</tr>


<tr>
<td><b>Total:</b></td>
<td><b>100</b></td>
</tr>

</table>
"""